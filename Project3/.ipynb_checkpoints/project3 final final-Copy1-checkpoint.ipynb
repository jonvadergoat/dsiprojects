{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement  we are trying to webscrape 2 subreddit posts that have different words and build a classifier model to predict the model to find out how accurately can it predict words to be from a Tinder or OKcupid subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build it on tinder subreddit and okccupid subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3 1. Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models using Natural Language Processing (NLP)\n",
    "1)Webscraping\n",
    "2)Data Cleaning\n",
    "3)Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Two different subreddits have been selected to be classified. They are namely:\n",
    "\n",
    "1) POSTS From OKCUPID\n",
    "2) POSTs From TINDER\n",
    "\n",
    "We are trying to to find a model thats most accurate in predicting text that from either OKCUPID or tinder.\n",
    "\n",
    "This will allow stakeholders to find out their apps are being talked more on subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping using Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELECT SUBREDDITS OKCUPID and TINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFR is okcupid\n",
    "#tfts is tinder\n",
    "\n",
    "TFR = 'https://www.reddit.com/r/OkCupid/.json'\n",
    "TFTS = 'https://www.reddit.com/r/Tinder/.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(url,headers = {'User-agent':'Bleep borp bot 1.0'},loops=2):\n",
    "    posts = []\n",
    "    names = []\n",
    "    titles = []\n",
    "    subreddit = []\n",
    "    aft_name=None\n",
    "\n",
    "    for i in range(loops):\n",
    "        if aft_name==None:\n",
    "            params={}\n",
    "        else:\n",
    "            params={'after':aft_name}\n",
    "\n",
    "        req = requests.get(url,params=params,headers=headers)\n",
    "\n",
    "        if req.status_code == 200:\n",
    "            the_json = req.json()\n",
    "            for p in range(len(the_json['data']['children'])):\n",
    "                names.append(the_json['data']['children'][p]['data']['name'])\n",
    "                titles.append(the_json['data']['children'][p]['data']['title'])\n",
    "                posts.append(the_json['data']['children'][p]['data']['selftext'])\n",
    "                subreddit.append(the_json['data']['children'][p]['data']['subreddit'])\n",
    "                aft_name = the_json['data']['after']\n",
    "        else:\n",
    "            print(res.status_code)\n",
    "            break\n",
    "            \n",
    "        time.sleep(np.random.randint(1,5))\n",
    "    \n",
    "    posts_df = pd.DataFrame({'names':names,\n",
    "                         'titles':titles,\n",
    "                         'posts':posts,\n",
    "                         'subreddit':subreddit},columns = ['names','titles','posts','subreddit'])\n",
    "    \n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df = get_posts(TFR,loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFTS_df = get_posts(TFTS,loops=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(TFR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TFTS_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates as Reddit API limits requests of post to 1000 only and checking how much data is collected:\n",
    "combined the titles and posts  and dropped duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df['text'] = TFR_df['titles'] + TFR_df['posts']\n",
    "TFTS_df['text'] = TFTS_df['titles'] + TFTS_df['posts']\n",
    "TFR_df = TFR_df.drop_duplicates(subset='text',keep='first')\n",
    "TFTS_df = TFTS_df.drop_duplicates(subset='text',keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TFR_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TFTS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFTS_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df.to_csv ('TFR_df.csv')\n",
    "TFTS_df.to_csv('TFTS_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3 2. Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification models using Natural Language Processing (NLP)Â¶\n",
    "2.1 Data Reading/Cleaning\n",
    "2.2 Exploratory Data Analysis\n",
    "2.3 Adressing stopwords, Lemmetization/Stemming\n",
    "2.4 Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Importing relevant libraries for data cleaning and EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df = pd.read_csv('TFR_df.csv')\n",
    "TFTS_df = pd.read_csv('TFTS_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFTS_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no null values so we can proceed on in the next step of our Data Cleaning!\n",
    "\n",
    "Now, both dataframes will be combined into one singular data frame, then cleaned and shuffled so that we can use it for classification later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([TFR_df,TFTS_df]).reset_index(drop=True)\n",
    "combined_df = shuffle(combined_df).reset_index(drop=True)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values for posts of tinder where people only posted about titles and not posts\n",
    "\n",
    "combined_df['posts'] = combined_df['posts'].fillna('')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply lower case to all the words in subreddit.\n",
    "combined_df['subreddit'] = combined_df['subreddit'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis\n",
    "Then we can check for the % of subreddit posts from each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.subreddit.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Next, the data in the titles and posts columns should be cleaned and tokenized so that we can train our classification model after. To do that, the function below is written to be iterated through our combined dataframe to remove all unwanted things such as html tags, spaces and punctuation etc and then split into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to clean text with regex and split words\n",
    "\n",
    "def string_clean(text):\n",
    "    \n",
    "    # Remove all HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove non-letters.\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # Remove all URL tags\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',text)\n",
    "    \n",
    "    # Keep text without punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # split text into a list of words\n",
    "    token_text = re.split('\\W+',text)\n",
    "    \n",
    "    return token_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead and apply this function to both our posts and titles to return us the tokens of each corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize posts and titles\n",
    "combined_df['p_tokenized'] = combined_df['posts'].apply(lambda x: string_clean(x))\n",
    "combined_df['t_tokenized'] = combined_df['titles'].apply(lambda x: string_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the combined df\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the columns are not ordered properly and it is hard to visualize these data thus we rearrange the order of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the order of columns\n",
    "combined_df = combined_df[['names','titles','t_tokenized','posts','p_tokenized','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better organized now! However, we can furthur simplify our data by removing stopwords! Similar to what we did above, we will write a function to do this and apply it to our newly created tokenized columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Adressing stopwords, Lemmetization/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for removing stop words and using stopwords\n",
    "def remove_stop_words(text):\n",
    "    \n",
    "    en_stopwords = list(nltk.corpus.stopwords.words('english'))\n",
    "    text = [word for word in text if word not in en_stopwords]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['p_stopped'] = combined_df['p_tokenized'].apply(lambda x: remove_stop_words(x))\n",
    "combined_df['t_stopped'] = combined_df['t_tokenized'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "\n",
    "# Rearranging columns for visibility again\n",
    "combined_df = combined_df[['names','titles','t_tokenized','t_stopped','posts','p_tokenized','p_stopped','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we can simplify our data one more time by lemmatizing our word list.\n",
    "\n",
    "Lemmatizing and Stemming are two forms of shortening words so we can combine similar forms of the same word. When we \"lemmatize\" data, we take words and attempt to return their lemma, or the base/dictionary form of a word e.g. destroying when lemmatized becomes destroy.\n",
    "\n",
    "First, we import and intantiate our lemmatizer and stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing wordlemmatizer and stemmer to stem and lemmatize our words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we write a function again to apply our lemmatizer and stemmer to our combined dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to stemmatize words\n",
    "def lemmatize_stemmer(text):\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    text = [p_stemmer.stem(i) for i in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df['p_final'] = combined_df['p_stopped'].apply(lambda x: remove_stop_words(x))\n",
    "combined_df['t_final'] = combined_df['t_stopped'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "# Rearranging columns for visibility again\n",
    "combined_df = combined_df[['names','titles','t_tokenized','t_stopped','t_final','posts','p_tokenized','p_stopped','p_final','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['final_combined'] = combined_df['t_final'] + combined_df['p_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now finally save this into a csv file so that it can be used in our final notebook where we will be doing our modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined df to a csv file\n",
    "\n",
    "combined_df.to_csv('combined_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Visualizations (Extra)\n",
    "Defining some functions to help us in our visualizations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(data):\n",
    "    word_counts = {}\n",
    "    words = data.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_dict_counts:\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a start, we plot the WordCloud image for the top 100 words that we see in our posts from OKCUPID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFR_text = ' '\n",
    "\n",
    "en_stopwords = list(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for x in combined_df[combined_df['subreddit'] == 'okcupid']['final_combined']: \n",
    "    for words in x: \n",
    "        TFR_text = TFR_text + words + ' '\n",
    "\n",
    "TFR_wc = WordCloud(max_words= 100,\n",
    "                      width = 1000, \n",
    "                      height = 750,\n",
    "                      background_color ='black',\n",
    "                      stopwords=en_stopwords, \n",
    "                      contour_width=3, \n",
    "                      contour_color='red',\n",
    "                      min_font_size = 10).generate(TFR_text) \n",
    "                      \n",
    "plt.figure(figsize = (20, 15)) \n",
    "plt.imshow(TFR_wc) \n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count top 40 words  in okcupid subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "top_N = 40\n",
    "\n",
    "df = pd.read_csv('TFR_df.csv',\n",
    "                 usecols=['subreddit','posts'])\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# RegEx for stopwords\n",
    "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "# replace '|'-->' ' and drop all stopwords\n",
    "words = (df.posts\n",
    "           .str.lower()\n",
    "           .replace([r'\\|', RE_stopwords], [' ', ''], regex=True)\n",
    "           .str.cat(sep=' ')\n",
    "           .split()\n",
    ")\n",
    "\n",
    "# generate DF out of Counter\n",
    "rslt = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)\n",
    "\n",
    "# plot\n",
    "rslt.plot.bar(rot=0, figsize=(16,10), width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count top 40 words in tinder subreddit df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "top_N = 40\n",
    "\n",
    "df = pd.read_csv('TFTS_df.csv',\n",
    "                 usecols=['subreddit','posts'])\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# RegEx for stopwords\n",
    "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "# replace '|'-->' ' and drop all stopwords\n",
    "words = (df.posts\n",
    "           .str.lower()\n",
    "           .replace([r'\\|', RE_stopwords], [' ', ''], regex=True)\n",
    "           .str.cat(sep=' ')\n",
    "           .split()\n",
    ")\n",
    "\n",
    "# generate DF out of Counter\n",
    "rslt = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)\n",
    "\n",
    "# plot\n",
    "rslt.plot.bar(rot=0, figsize=(16,10), width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like , people , get ,anyone , said , profile words that appear both in the top 40 okcupid and tinder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we place the entire list of words and their counts into a DataFrame and proceed to sort them in descending order so that we can plot the corresponding bar plot out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can do the same for our other subreddit, TINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFTS_text = ' '\n",
    "\n",
    "en_stopwords = list(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "for x in combined_df[combined_df['subreddit'] == 'tinder']['final_combined']: \n",
    "    for words in x: \n",
    "        TFTS_text = TFTS_text + words + ' '\n",
    "\n",
    "TFTS_wc = WordCloud(max_words= 100,\n",
    "                      width = 1000, \n",
    "                      height = 750,\n",
    "                      background_color ='black',\n",
    "                      stopwords=en_stopwords.extend(['x200b','amp']), \n",
    "                      contour_width=3, \n",
    "                      contour_color='red',\n",
    "                      min_font_size = 10).generate(TFTS_text) \n",
    "                      \n",
    "plt.figure(figsize = (20, 15)) \n",
    "plt.imshow(TFTS_wc) \n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now , we will remove the common words of the top 40 words that are similar in OKCUPID and TINDER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to remove the common words in the top 40 of OKCUPID and TINDER from our combined data frame.\n",
    "\n",
    "def cleaner2(final_posts):\n",
    "    stopwords2 = ['like' , 'people' , 'get' ,'anyone' , 'said', 'profile','one','someone','really']\n",
    "    cleanser = [x for x in final_posts if x not in stopwords2]\n",
    "    return(cleanser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['final_combined'] = combined_df['final_combined'].map(cleaner2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 3 3. Modelling and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we will be running several different models with different vectorizers and will be cross comparing between their training and testing scores to see how their scores compare up to each other.\n",
    "\n",
    "Models:\n",
    "â 1. Logistic Regression Model and Count Vectorizer\n",
    "â 2. MultinomialNB Model and Count Vectorizer\n",
    "â 3. Logistic Regression Model and TfidfVectorizer\n",
    "â 4. MultinomialNB Model and TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, a Final Modelling Run will be run with the various best parameters of all the models and the corresponding scores, and some data between models will be generated for comparision, and then everything will be tabulated into a table for easy cross comparision and visualization.\n",
    "\n",
    "Importing the relevant libraries for modelling and classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive-Bayes Classifier (Multinomial Model) with Count-Vectorizer\n",
    "First, we turn subreddit into a 1/0 column, where 1 indicates Okcupid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['okcupid'] = [1 if df1.loc[i,'subreddit'] == 'okcupid' else 0 for i in range(df1.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['okcupid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['final_combined']\n",
    "y = df1['okcupid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE SCORE of the two subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then can instantiate CountVectorizer and instantiate our NB pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('cvec', CountVectorizer()), \n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go on to predict y using X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_ypred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can check our scores against our training and testing sets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "naive_bayes.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "naive_bayes.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both our training and testing scores are similar and does not show a little over-fitting  However, we can use GridSearchCV to help us search for the best parameters for our NB model in the next portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE are selecting MultinomialNB \n",
    "and why?\n",
    "The columns of X are all integer counts, so MultinomialNB is the best choice here.\n",
    "BernoulliNB is best when we have 0/1 counts in all columns of X. (a.k.a. dummy variables)\n",
    "GaussianNB is best when the columns of X are Normally distributed. (Practically, though, it gets used whenever BernoulliNB and MultinomialNB are inappropriate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Naive-Bayes Classifier (Multinomial Model) GridSearchCV, Count-Vectorizer\n",
    "First, we initialize the pipe parameters to be fed into our GridSearchCV as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.9, .95],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = GridSearchCV(naive_bayes,\n",
    "                       param_grid=pipe_params,\n",
    "                       scoring='accuracy'\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting our training data to GridSearchCV model...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(nb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now instantiate NB model with best parameters for CountVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('cvec', CountVectorizer(ngram_range = (1,2),\n",
    "                                                 max_features = 3000,\n",
    "                                                 max_df = 0.9,\n",
    "                                                 min_df = 2)), \n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score on training set: {naive_bayes.score(X_train, y_train)}')\n",
    "print(f'Score on testing set: {naive_bayes.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive-Bayes Classifier (Multinomial Model) GridSearchCV, Tfidf-Vectorizer\n",
    "Here we use Tfidf-Vectorizer instead of Count-Vectorizer and see what impact it has on our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use TF-IDF?\n",
    "Common words are penalized.\n",
    "Rare words have more influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('vector',TfidfVectorizer()), \n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vector__max_df':[0.9,0.95],\n",
    "    'vector__min_df':[0.0001,0.001,0.01],\n",
    "    'vector__ngram_range':[(1,1),(1,2),(1,3),(1,4),(1,5)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = GridSearchCV(naive_bayes,\n",
    "                       param_grid=pipe_params,\n",
    "                       scoring='accuracy'\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, we will use default values for our BaseLine NB model first.\n",
    "\n",
    "We will fit our training data first to our NB pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar to above, we instantiate a new NB model with our best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Pipeline([('vector',TfidfVectorizer(ngram_range=(1,2),\n",
    "                                                     min_df=0.01,\n",
    "                                                     max_df=0.9)),\n",
    "                            ('multi_nb', MultinomialNB())\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GridSearch for Best Parameters:\n",
    "And then we can proceed to use this GridSearch function below to find our best parameters for the different models we will be initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1['okcupid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['final_combined']\n",
    "y = df1['okcupid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_searcher(X, y, vectorizer, model):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.25,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "    \n",
    "    full_name_dict = {'cvec' : 'Count Vectorizer',\n",
    "                      'tvec' : 'TfidfVectorizer',\n",
    "                      'multi_nb' : 'MultinomialNB',\n",
    "                      'lr' : 'Logistic Regression',\n",
    "                      'dt' : 'Decision Tree Classifier',\n",
    "                      'rf': 'Random Forest Classifier'}\n",
    "    \n",
    "    vec_dict =  {'cvec': CountVectorizer(),\n",
    "                 'tvec': TfidfVectorizer()\n",
    "                }\n",
    "    \n",
    "    param_dict = {'cvec': {'cvec__max_features': [2500, 3500],\n",
    "                           'cvec__min_df': [2, 3],\n",
    "                           'cvec__max_df': [.9, .95],\n",
    "                           'cvec__ngram_range': [(1,1), (1,2)]},\n",
    "                  'tvec': {'tvec__max_features': [2500,3000,3500],\n",
    "                           'tvec__min_df':[2,3],\n",
    "                           'tvec__max_df':[.9,.95],\n",
    "                           'tvec__ngram_range':[(1,1),(1,2)]},\n",
    "                  'dt' : {'dt__max_depth': [3,5],\n",
    "                          'dt__min_samples_split': [5,10],\n",
    "                          'dt__min_samples_leaf': [2,3]},\n",
    "                  'rf' : {'rf__n_estimators': [100],\n",
    "                          'rf__max_depth': [None, 1, 2],\n",
    "                          'rf__min_samples_split': [5,10],\n",
    "                          'rf__min_samples_leaf': [2,3]},\n",
    "                  'lr' : {},\n",
    "                  'multi_nb' : {}\n",
    "                 }\n",
    "\n",
    "    model_dict = {'multi_nb' : MultinomialNB(),\n",
    "                  'lr' : LogisticRegression(),\n",
    "                  'dt' : DecisionTreeClassifier(),\n",
    "                  'rf' : RandomForestClassifier()\n",
    "                  }\n",
    "    \n",
    "    pipe = Pipeline([(vectorizer, vec_dict[vectorizer]), \n",
    "                    ((model, model_dict[model]))])\n",
    "    \n",
    "    \n",
    "    param_dict[model].update(param_dict[vectorizer])\n",
    "    pipe_params = param_dict[model]\n",
    "    \n",
    "    grid = GridSearchCV(pipe,\n",
    "           param_grid=pipe_params,\n",
    "           cv=3)\n",
    "        \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'Using {full_name_dict[model]} Model and {full_name_dict[vectorizer]}:')\n",
    "    print(f'Model train score : {grid.best_score_}')\n",
    "    print(f'Model test score : {grid.score(X_test,y_test)}')\n",
    "    print(f'Model best params : {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GridSearchCV for Logistic Regression and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher(X,y,'cvec','lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GridSearchCV for MultinomialNB and CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher(X,y,'cvec','multi_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV for Logistic Regression and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher(X,y,'tvec','lr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GridSearchCV for MultinomialNB and TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher(X,y,'tvec','multi_nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with CountVectorizer semes to give the best training and test scores.\n",
    "MultinomialNB model together with CountVectorizer seems to give the best training and test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Modelling Runs:\n",
    "Finally, we can use our best parameters found using GridSearch to initialize various models with best parameters and run our predictions on our test data and interpret the results correspondingly below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final Logistic Regression with CountVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                        ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "lr_cvec_ypred_fin = lr_cvec_fin.predict(X_test)\n",
    "lr_cvec_fin_acc = accuracy_score(y_test,lr_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {lr_cvec_fin_acc}')\n",
    "print(classification_report(y_test,lr_cvec_ypred_fin,target_names=['okcupid','tinder']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "print('The Accuracy rate is '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (fp+fn)/(tn+fp+fn+tp)\n",
    "print('The Error rate is '+ str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(lr_cvec_fin , X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(lr_cvec_fin, X_test, y_test);\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since `okcupid` = 1, in this case:\n",
    "\n",
    "183 posts were correctly predicted by this model to be from Okcupid.\n",
    "20posts were wrongly predicted by this model to be from OKcupid.\n",
    "44posts were wrongly predicted by this model to be from tinder.\n",
    "160 posts were correctly predicted by this model to be from tinder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final Logistic Regression with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 1))),\n",
    "                        ('lr', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_cvec_ypred_fin = lr_cvec_fin.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "lr_tvec_ypred_fin = lr_tvec_fin.predict(X_test)\n",
    "lr_tvec_fin_acc = accuracy_score(y_test,lr_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {lr_tvec_fin_acc}')\n",
    "print(classification_report(y_test,lr_tvec_ypred_fin,target_names=['okcupid','tinder']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_tvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "print('The Accuracy rate is '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (fp+fn)/(tn+fp+fn+tp)\n",
    "print('The Error rate is '+ str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(lr_tvec_fin , X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(lr_tvec_fin, X_test, y_test);\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auc = roc_auc_score(y_test,  lr_tvec_ypred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "101 posts were correctly predicted by this model to be from OKCUPID.\n",
    "10 posts were wrongly predicted by this model to be from OKCUPID.\n",
    "3 posts were wrongly predicted by this model to be from Tinder.\n",
    "241 posts were correctly predicted by this model to be from Tinder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Final MultinomialNB with CountVectorizer model:Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, lr_cvec_ypred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cvec_fin = Pipeline([('cvec',CountVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 2,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                        ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_cvec_fin.fit(X_train,y_train)\n",
    "\n",
    "nb_cvec_ypred_fin = nb_cvec_fin.predict(X_test)\n",
    "nb_cvec_fin_acc = accuracy_score(y_test,nb_cvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {nb_cvec_fin_acc}')\n",
    "print(classification_report(y_test,nb_cvec_ypred_fin,target_names=['okcupid','tinder']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, nb_cvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "print('The Accuracy rate is '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (fp+fn)/(tn+fp+fn+tp)\n",
    "print('The Error rate is '+ str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_confusion_matrix(nb_cvec_fin , X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(nb_cvec_fin, X_test, y_test);\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test,  nb_cvec_ypred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are trying to identify posts with words that belong to either 'OKcupid' or 'Tinder' subreddit. After webscraping the subreddits for data, we preprocessed it and we built a Logistic Regression model as well as a Multinomial Naive Bayes model to train the data. Both models performed fairly well with 0.86 accuracy rate for Logistic Regression and 0.76 for Multinomial Naive Bayes model but we select the Logistic Regression Model with Count Vectorizer to accurately predict our text whether it comes from the OKCUPID thread as it has a high accuracy rate of 86%.\n",
    "\n",
    "For the Receiver Operating Characteristic Curve above, we want the yellow line to be as far away from the blue line as much as possible which is also represented as the Area Under Curve. The AUC for the above ROC is 0.84 which means that it is quite accurate for our LOGISTIC regression model and does better than our Multinomial Naive Bayes model of 0.74."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MultinomialNB with TfidfVectorizer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tvec_fin = Pipeline([('tvec',TfidfVectorizer(max_df = 0.9, \n",
    "                                                max_features = 2500, \n",
    "                                                min_df = 3,\n",
    "                                                ngram_range = (1, 2))),\n",
    "                         ('multi_nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tvec_fin.fit(X_train,y_train)\n",
    "\n",
    "nb_tvec_ypred_fin = nb_tvec_fin.predict(X_test)\n",
    "nb_tvec_fin_acc = accuracy_score(y_test,nb_tvec_ypred_fin)\n",
    "\n",
    "print(f'Accuracy: {nb_tvec_fin_acc}')\n",
    "print(classification_report(y_test,nb_tvec_ypred_fin,target_names=['Okcupid','Tinder']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, nb_tvec_ypred_fin).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "print('The Accuracy rate is '+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (fp+fn)/(tn+fp+fn+tp)\n",
    "print('The Error rate is '+ str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_confusion_matrix(nb_tvec_fin , X_test, y_test, cmap='Blues', values_format='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "plot_roc_curve(nb_tvec_fin, X_test, y_test);\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 200),\n",
    "         np.linspace(0, 1, 200),\n",
    "         label='baseline',\n",
    "         linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the wrongly classified posts\n",
    "\n",
    "we put the multinomial naive bayes model with TFID vectorizr model and see the wrongly predicted posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df=pd.DataFrame(y_test)\n",
    "pred_df[\"text\"]=X_test\n",
    "pred_df['pred']=nb_tvec_ypred_fin\n",
    "#pred_df.loc[pred_df['subreddit']!=pred_df['pred'],:]\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.loc[pred_df['okcupid']!=pred_df['pred'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[540,'final_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[609,'final_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.loc[pred_df['okcupid']!=pred_df['pred'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"pred_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Models Score Tabulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the model of logistic regression with count vectorizer as it has the highest accuracy and best ROC AUC score of 81% accuracy of predicting okcupid and 0.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use logistic regression with countvectorizer to predict if a person talking is talking towards more long term relationship dating than short term dating.\n",
    "\n",
    "this is done by predicting the posts relative to Okcupid and assuming okcupid subreddits talk more about relationship material\n",
    "\n",
    "We could also do sentiment analysis and do more testing on different posts about relationship and short term dating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
